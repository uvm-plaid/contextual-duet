-- gradient descent
-- output perturbation
-- a la Wu et al.
-- per-iteration bound
-- https://arxiv.org/abs/1606.04722

let main =  ∀ m  : ℕ,
              n  : ℕ,
              ε  : ℝ⁺,
              k  : ℕ,
              δ  : ℝ⁺,
              δ′ : ℝ⁺
              .
           pλ xs : 𝕄 [L∞, U|m, n↦𝔻 ] ⋅ 1.0 ,
              ys : 𝕄 [L∞, U|m, 1.0↦𝔻 ] ⋅ 1.0,
              ε  : ℝ⁺[ε] ⋅ 1.0,
              k  : ℕ[k] ⋅ 1.0,
              δ  : ℝ⁺[δ] ⋅ 1.0,
              δ′ : ℝ⁺[δ′] ⋅ 1.0,
              η  : ℝ ⋅ 1.0,
              m  : ℕ[m] ⋅ 1.0,
              n  : ℕ[n] ⋅ 1.0
              ⇒
  let colnum = (mcols @ m @ n @ 𝔻 xs) in
  let m₀ = mcreate @ ℕ[1.0] @ ℕ[n] @ ℝ ℕ[1] colnum (sλ i : 𝕀[1.0] , j : 𝕀[n] ⇒ 0.0) in
  -- let s = (real₁ k) / real₁ (mrows @ m @ n @ 𝔻 xs) in
  -- other loop?
  t ← (aloop₁ @ ε @ δ @ δ′ @ k @ n @ <xs,ys>
        (pλ <xs,ys> θ : 𝕄 [L∞, U|1.0, n↦𝔻 ] ⋅ 1.0 ⇒
            let g = mLipGrad @ m @ n θ (mclip @ m @ n @ 𝔻 xs) ys in
            return msub @ ℕ[1.0] @ n (mconv @ ℕ[1.0] @ n θ) (mscale₁ @ ℕ[1.0] @ n g η)
          ) m₀ δ′);
  g ← mgauss @ ε @ δ @ ℕ[1.0] @ n ε δ t;
  return g

in main
