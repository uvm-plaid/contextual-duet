-- gradient descent
-- per-iteration bound
-- with minibatching
let main = ∀  m  : ℕ,
              n  : ℕ,
              ε  : ℝ⁺,
              k  : ℕ,
              δ  : ℝ⁺,
              δ′ : ℝ⁺,
              b  : ℕ
              .
          pλ  xs : 𝕄 [L∞, U|m, n↦𝔻 ] ⋅ 1.0 ,
              ys : 𝕄 [L∞, U|m, 1.0↦𝔻 ] ⋅ 1.0 ,
              ε  : ℝ⁺[ε] ⋅ 1.0,
              k  : ℕ[k] ⋅ 1.0,
              δ  : ℝ⁺[δ] ⋅ 1.0,
              δ′ : ℝ⁺[δ′] ⋅ 1.0,
              η  : ℝ ⋅ 1.0,
              b  : ℕ[b] ⋅ 1.0
              ⇒
  let d = mcols @ m @ n @ 𝔻 xs in
  let m₀ = mcreate @ ℕ[1.0] @ ℕ[n] @ ℝ ℕ[1] d (sλ i : 𝕀[1.0] , j : 𝕀[n] ⇒ 0.0) in
  aloop @ ℝ⁺[2.0⋅(b/m)⋅ε] @ ℝ⁺[(b/m)⋅δ] @ δ′ @ k @ n @ <xs,ys>
    (pλ <xs,ys> θ : 𝕄 [L∞, U|1.0, n↦𝔻 ] ⋅ 1.0 ⇒
      g ← sample @ ε @ δ @ b @ m @ n @ 𝕄 [L∞, U|1.0, n↦ℝ ] b
        (pλ <xs',ys'> xs' : 𝕄 [L∞, U|b, n↦𝔻 ] ⋅ 1.0, <xs',ys'> ys' : 𝕄 [L∞, U|b, 1.0↦𝔻 ] ⋅ 1.0  ⇒
          mgauss @ ε @ δ @ ℕ[1.0] @ n ε δ
            (mLipGrad @ b @ n θ (mclip @ b @ n @ 𝔻 xs') ys'))
        (mclip @ m @ n @ 𝔻 xs) ys;
      return (matsub @ ℕ[1.0] @ n θ g))
    m₀
    δ′

in main
